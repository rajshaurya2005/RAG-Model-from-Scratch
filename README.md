# **SimpleRAG: A Basic Retrieval Augmented Generation System**

This Python script implements a simple Retrieval Augmented Generation (RAG) system. It allows you to add documents (PDF, TXT, JSON), process their content, and then ask questions to get answers generated by a Large Language Model (LLM) based on the information retrieved from those documents.

This system uses:

* **Groq** for fast LLM inference.  
* **Sentence Transformers** for creating text embeddings.  
* **ChromaDB** as a vector store for efficient similarity search.  
* **PyPDF2** for extracting text from PDF files.

## **Features**

* Supports ingesting documents in PDF, TXT, and JSON formats.  
* Chunks text into manageable pieces with configurable overlap.  
* Generates embeddings for text chunks and stores them.  
* Retrieves relevant chunks based on user questions using vector similarity.  
* Generates responses using Groq's LLMs (e.g., Llama 3, Mixtral) based on the retrieved context.  
* Highly configurable:  
  * Groq API Key  
  * Embedding model (from Sentence Transformers library)  
  * Text chunk size and overlap  
  * Groq LLM model for generation  
  * Generation parameters (temperature, max tokens)  
  * Number of retrieved context chunks

## **Prerequisites**

* Python 3.7+  
* A Groq API Key. You can get one from [GroqCloud](https://console.groq.com/keys).

## **Installation**

1. Clone the repository or download the script:  
   Save the Python code as simple\_rag.py (or your preferred filename).  
2. **Install** the required Python **libraries:**  
   pip install groq sentence-transformers chromadb pypdf2

   You might want to create a requirements.txt file:  
   \# requirements.txt  
   groq  
   sentence-transformers  
   chromadb  
   PyPDF2

   And then install using:  
   pip install \-r requirements.txt

## **Configuration**

You need to provide your Groq API key when instantiating the SimpleRAG class.

from simple\_rag import SimpleRAG \# Assuming you saved the code as simple\_rag.py

rag \= SimpleRAG(groq\_api\_key="your-groq-api-key")

Replace "your-groq-api-key" with your actual Groq API key.

## **Usage**

Here's how to use the SimpleRAG system:

1. **Import and Initialize:**  
   from simple\_rag import SimpleRAG \# Or the name you saved your file as

   \# Initialize with your Groq API key  
   api\_key \= "your-groq-api-key"  
   rag\_system \= SimpleRAG(groq\_api\_key=api\_key)

2. Add a Document:  
   Make sure you have a document (e.g., my\_document.pdf, data.txt, info.json) in the same directory or provide the correct path.  
   file\_path \= "path/to/your/document.pdf" \# Replace with your document  
   chunks\_added \= rag\_system.add\_document(file\_path)  
   if chunks\_added \> 0:  
       print(f"Successfully added {chunks\_added} chunks from {file\_path}")  
   else:  
       print(f"Could not process or found no text in {file\_path}")

3. Generate a Response:  
   Ask a question related to the content of the added document(s).  
   question \= "What is the main topic discussed in this document?"  
   answer \= rag\_system.generate\_response(question)  
   print(f"\\nQuestion: {question}")  
   print(f"Answer: {answer}")

### **Example with Custom Parameters**

You can customize various aspects of the RAG pipeline:

from simple\_rag import SimpleRAG

\# Initialize with custom settings  
custom\_rag \= SimpleRAG(  
    groq\_api\_key="your-groq-api-key",  
    embedding\_model="all-mpnet-base-v2",  \# A different embedding model  
    chunk\_size=300,                       \# Smaller chunk size  
    chunk\_overlap=30                      \# Smaller overlap  
)

\# Add a document  
custom\_rag.add\_document("path/to/your/other\_document.txt") \# Replace with your document

\# Generate a response with custom generation parameters  
custom\_question \= "Summarize the key findings."  
custom\_answer \= custom\_rag.generate\_response(  
    question=custom\_question,  
    model\_name="mixtral-8x7b-32768",  \# Use Mixtral model on Groq  
    temperature=0.3,                  \# Adjust creativity  
    max\_tokens=2048,                  \# Allow for longer responses  
    n\_results=5                       \# Retrieve more context chunks  
)

print(f"\\nQuestion: {custom\_question}")  
print(f"Answer: {custom\_answer}")

## **Class Overview:** SimpleRAG

### \_\_init\_\_(self, groq\_api\_key, embedding\_model="all-MiniLM-L6-v2", chunk\_size=500, chunk\_overlap=50)

* Initializes the RAG system.  
* **Parameters:**  
  * groq\_api\_key (str): Your Groq API key.  
  * embedding\_model (str, optional): The name of the Sentence Transformer model to use. Defaults to "all-MiniLM-L6-v2".  
  * chunk\_size (int, optional): The number of words per text chunk. Defaults to 500.  
  * chunk\_overlap (int, optional): The number of words to overlap between consecutive chunks. Defaults to 50.

### add\_document(self, file\_path)

* Reads a document, chunks its text, generates embeddings, and stores them.  
* **Parameters:**  
  * file\_path (str): The path to the document (PDF, TXT, or JSON).  
* **Returns:**  
  * int: The number of chunks added from the document. Returns 0 if the file is empty or unsupported.

### generate\_response(self, question, model\_name="llama3-8b-8192", temperature=0.1, max\_tokens=1024, n\_results=3)

* Retrieves relevant context for the question and generates an answer using an LLM.  
* **Parameters:**  
  * question (str): The question to ask.  
  * model\_name (str, optional): The Groq LLM model to use for generation. Defaults to "llama3-8b-8192".  
  * temperature (float, optional): Controls the randomness of the LLM's output. Lower is more deterministic. Defaults to 0.1.  
  * max\_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 1024.  
  * n\_results (int, optional): The number of relevant document chunks to retrieve as context. Defaults to 3.  
* **Returns:**  
  * str: The generated answer. Returns "No relevant information found." if no context is retrieved.

## **Internal Methods (for reference)**

* \_read\_file(self, path): Reads text content from PDF, TXT, or JSON files.  
* \_chunk\_text(self, text): Splits the input text into overlapping chunks based on self.chunk\_size and self.chunk\_overlap.

## **Further Customization**

* **Embedding Models:** Explore other models from the [Sentence Transformers library](https://www.sbert.net/docs/pretrained_models.html) for different performance/quality trade-offs.  
* **Groq LLM Models:** Check the [Groq documentation](https://console.groq.com/docs/models) for available models and their capabilities.  
* **ChromaDB:** For more advanced use cases, you can explore ChromaDB's features for persisting collections, using different distance metrics, etc.

## **License**

This script is provided as-is. You are free to use, modify, and distribute it.
